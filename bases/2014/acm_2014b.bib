@inproceedings{10.1145/2676662.2676676,
author = {Samreen, Faiza and Blair, Gordon S. and Rowe, Matthew},
title = {Adaptive Decision Making in Multi-Cloud Management},
year = {2014},
isbn = {9781450332330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676662.2676676},
doi = {10.1145/2676662.2676676},
abstract = {The more cloud providers in the market the more information users have to handle to choose the best and suitable option for their application or business. The diversity in cloud services is a challenge for automated decision making in the multi-cloud environment. These decisions become more complex when the application's requirements and the application owner's constraints need to be satisfied throughout the application life cycle.This paper presents the concept of an Adaptive Decision Making Broker (ADMB) for multi-cloud management. ADMB aims to provide multi-criteria decision making using machine learning in a multi-cloud environment. In this context, we believe that our proposed methodology has the potential to provide optimal solutions as well as handle trade-offs between the functional and the non-functional requirements of given application.},
booktitle = {Proceedings of the 2nd International Workshop on CrossCloud Systems},
articleno = {4},
numpages = {6},
keywords = {multi-criteria decision making, machine learning, multi-cloud management},
location = {Bordeaux, France},
series = {CCB '14}
}

@inproceedings{10.1145/2668260.2668277,
author = {Elomda, Basem Mohamed and Hefny, Hesham Ahmed and Hazman, Maryam and Hassan, Hesham Ahmed},
title = {An Enhanced Method for MCDM Based on Improved Fuzzy Decision Map},
year = {2014},
isbn = {9781450327671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668260.2668277},
doi = {10.1145/2668260.2668277},
abstract = {Fuzzy Decision Map (FDM) method was proposed in 2006 to solve multicriteria decision making problems (MCDM) having dependence and feedback among criteria. FDM provides a simpler approach to solve such problems compared with the method of Analytical Network Process (ANP) developed in 1996. ANP is a general form of the Analytic Hierarchy Process (AHP). On the other hand, FDM is only employed to derive evaluation criteria. It can't be used for ranking alternatives as usually required in many real world MCDM problems. The aim of this work is to enhance the FDM structure to be able to make a proper decision for complex decision making problem by selecting best alternative. The enhanced FDM method should take into consideration the alternative level. A case study was carried out to demonstrate the proposed model.},
booktitle = {Proceedings of the 6th International Conference on Management of Emergent Digital EcoSystems},
pages = {56–61},
numpages = {6},
keywords = {Fuzzy Decision Map, Multi Criteria Decision Making, Fuzzy Cognitive Map, Fuzzy Set theory, Soft Computing},
location = {Buraidah, Al Qassim, Saudi Arabia},
series = {MEDES '14}
}

@inproceedings{10.1145/2598394.2605339,
author = {Brockhoff, Dimo},
title = {GECCO 2014 Tutorial on Evolutionary Multiobjective Optimization},
year = {2014},
isbn = {9781450328814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598394.2605339},
doi = {10.1145/2598394.2605339},
abstract = {Many optimization problems are multiobjective in nature in the sense that multiple, conflicting criteria need to be optimized simultaneously. Due to the conflict between objectives, usually, no single optimal solution exists. Instead, the optimum corresponds to a set of so-called Pareto-optimal solutions for which no other solution has better function values in all objectives.Evolutionary Multiobjective Optimization (EMO) algorithms are widely used in practice for solving multiobjective optimization problems due to several reasons. As stochastic blackbox algorithms, EMO approaches allow to tackle problems with nonlinear, nondifferentiable, or noisy objective functions. As set-based algorithms, they allow to compute or approximate the full set of Pareto-optimal solutions in one algorithm run---opposed to classical solution-based techniques from the multicriteria decision making (MCDM) field. Using EMO approaches in practice has two other advantages: they allow to learn about a problem formulation, for example, by automatically revealing common design principles among (Pareto-optimal) solutions (innovization) and it has been shown that certain single-objective problems become easier to solve with randomized search heuristics if the problem is reformulated as a multiobjective one (multiobjectivization).This tutorial aims at giving a broad introduction to the EMO field and at presenting some of its recent research results in more detail. More specifically, we are going to (i) introduce the basic principles of EMO algorithms in comparison to classical solution-based approaches, (ii) show a few practical examples which motivate the use of EMO in terms of the mentioned innovization and multiobjectivization principles, and (iii) present a general overview of state-of-the-art algorithms and techniques. Moreover, we will present some of the most important research results in areas such as indicator-based EMO, preference articulation, and performance assessment.Though classified as introductory, this tutorial is intended for both novices and regular users of EMO. Those without any knowledge will learn about the foundations of multiobjective optimization and the basic working principles of state-of-the-art EMO algorithms. Open questions, presented throughout the tutorial, can serve for all participants as a starting point for future research and/or discussions during the conference.},
booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {297–322},
numpages = {26},
keywords = {tutorial, emo, multi-criteria decision making, evolutionary multiobjective optimization},
location = {Vancouver, BC, Canada},
series = {GECCO Comp '14}
}

@inproceedings{10.1145/2618243.2618246,
author = {Zhang, Ying and Zhang, Wenjie and Lin, Xuemin and Cheema, Muhammad Aamir and Zhang, Chengqi},
title = {Matching Dominance: Capture the Semantics of Dominance for Multi-Dimensional Uncertain Objects},
year = {2014},
isbn = {9781450327220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618243.2618246},
doi = {10.1145/2618243.2618246},
abstract = {The dominance operator plays an important role in a wide spectrum of multi-criteria decision making applications. Generally speaking, a dominance operator is a partial order on a set O of objects, and we say the dominance operator has the monotonic property regarding a family of ranking functions F if o1 dominates o2 implies f(o1) ≥ f(o2) for any ranking function f ∈ F and objects o1, o2 ∈ O. The dominance operator on the multi-dimensional points is well defined, which has the monotonic property regarding any monotonic ranking (scoring) function. Due to the uncertain nature of data in many emerging applications, a variety of existing works have studied the semantics of ranking query on uncertain objects. However, the problem of dominance operator against multi-dimensional uncertain objects remains open. Although there are several attempts to propose dominance operator on multi-dimensional uncertain objects, none of them claims the monotonic property on these ranking approaches.Motivated by this, in this paper we propose a novel matching based dominance operator, namely matching dominance, to capture the semantics of the dominance for multi-dimensional uncertain objects so that the new dominance operator has the monotonic property regarding the monotonic parameterized ranking function, which can unify other popular ranking approaches for uncertain objects. Then we develop a layer indexing technique, Matching Dominance based Band (MDB), to facilitate the top k queries on multi-dimensional uncertain objects based on the matching dominance operator proposed in this paper. Efficient algorithms are proposed to compute the MDB index. Comprehensive experiments convincingly demonstrate the effectiveness and efficiency of our indexing techniques.},
booktitle = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management},
articleno = {18},
numpages = {12},
location = {Aalborg, Denmark},
series = {SSDBM '14}
}

@inproceedings{10.1145/2637248.2637260,
author = {Aggarwal, Sonal and Van Oostendorp, Herre and Reddy, Y. Raghu and Indurkhya, Bipin},
title = {Providing Web Credibility Assessment Support},
year = {2014},
isbn = {9781450328746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2637248.2637260},
doi = {10.1145/2637248.2637260},
abstract = {Presence of information from multiple sources on the internet requires evaluating the credibility of the information, before its utilization. Researchers have suggested that internet users experience difficulty in accessing necessary information and do not pay enough attention to its credibility. We present here the design and implementation of an automated Web Credibility Assessment Support Tool (WebCAST) that considers multiple factors (type of website, popularity, sentiment, date of last update, reputation and review based on users' ratings reflecting personal experience) for assessing the credibility of information and returns a summary indication of the credibility of a website. We use Potentially All Pairwise RanKings of all possible Alternatives (PAPRIKA) method of Multi-Criteria Decision Analysis (MCDA) to give weights to the scale values on each factor, representing the relative importance of the attributes. An empirical evaluation of the tool was conducted by computing the correlation between the tool-generated credibility scores and that of human judges. The correlation was found to be 0.89, thus verifying the validity of the tool. In the future the proposed tool can be made useful to students in their learning process of credibility assessment.},
booktitle = {Proceedings of the 2014 European Conference on Cognitive Ergonomics},
articleno = {29},
numpages = {8},
keywords = {Web Trust, Support tool, Websites, Credibility, Automation},
location = {Vienna, Austria},
series = {ECCE '14}
}

@inproceedings{10.1145/2659522.2659526,
author = {Farnadi, Golnoosh and Sushmita, Shanu and Sitaraman, Geetha and Ton, Nhat and De Cock, Martine and Davalos, Sergio},
title = {A Multivariate Regression Approach to Personality Impression Recognition of Vloggers},
year = {2014},
isbn = {9781450331296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659522.2659526},
doi = {10.1145/2659522.2659526},
abstract = {Research in psychology has suggested that behavior of individuals can be explained to a great extent by their underlying personality traits. In this paper, we focus on predicting how the personality of YouTube video bloggers is perceived by their viewers. Our approach to personality recognition is multimodal in the sense that we use audio-video features, as well as textual (emotional and linguistic) features extracted from the transcripts of vlogs. Based on these features, we predict the extent to which the video blogger is perceived to exhibit each of the traits of the Big Five personality model. In addition, we explore 5 multivariate regression techniques and contrast them with a single target approach for predicting personality impression scores. All 6 algorithms are able to outperform the average baseline model for all 5 personality traits on a dataset of 404 YouTube videos. This is interesting because previously published methods for the same dataset show an improvement over the baseline for the majority of personality traits, but not for all simultaneously.},
booktitle = {Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition},
pages = {1–6},
numpages = {6},
keywords = {personality impression prediction, multimodal personality recognition, YouTube vlog data, big five personality model, multivariate regression},
location = {Orlando, Florida, USA},
series = {WCPR '14}
}

@article{10.1145/2566669,
author = {Panerati, Jacopo and Beltrame, Giovanni},
title = {A Comparative Evaluation of Multi-Objective Exploration Algorithms for High-Level Design},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2566669},
doi = {10.1145/2566669},
abstract = {This article presents a detailed overview and the experimental comparison of 15 multi-objective design-space exploration (DSE) algorithms for high-level design. These algorithms are collected from recent literature and include heuristic, evolutionary, and statistical methods. To provide a fair comparison, the algorithms are classified according to the approach used and examined against a large set of metrics. In particular, the effectiveness of each algorithm was evaluated for the optimization of a multiprocessor platform, considering initial setup effort, rate of convergence, scalability, and quality of the resulting optimization. Our experiments are performed with statistical rigor, using a set of very diverse benchmark applications (a video converter, a parallel compression algorithm, and a fast Fourier transformation algorithm) to take a large spectrum of realistic workloads into account. Our results provide insights on the effort required to apply each algorithm to a target design space, the number of simulations it requires, its accuracy, and its precision. These insights are used to draw guidelines for the choice of DSE algorithms according to the type and size of design space to be optimized.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {15},
numpages = {22},
keywords = {computer-aided manufacturing, Design automation, guidelines, Pareto optimization}
}

@article{10.1145/2532515,
author = {Zhu, Hengshu and Chen, Enhong and Xiong, Hui and Yu, Kuifei and Cao, Huanhuan and Tian, Jilei},
title = {Mining Mobile User Preferences for Personalized Context-Aware Recommendation},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2532515},
doi = {10.1145/2532515},
abstract = {Recent advances in mobile devices and their sensing capabilities have enabled the collection of rich contextual information and mobile device usage records through the device logs. These context-rich logs open a venue for mining the personal preferences of mobile users under varying contexts and thus enabling the development of personalized context-aware recommendation and other related services, such as mobile online advertising. In this article, we illustrate how to extract personal context-aware preferences from the context-rich device logs, or context logs for short, and exploit these identified preferences for building personalized context-aware recommender systems. A critical challenge along this line is that the context log of each individual user may not contain sufficient data for mining his or her context-aware preferences. Therefore, we propose to first learn common context-aware preferences from the context logs of many users. Then, the preference of each user can be represented as a distribution of these common context-aware preferences. Specifically, we develop two approaches for mining common context-aware preferences based on two different assumptions, namely, context-independent and context-dependent assumptions, which can fit into different application scenarios. Finally, extensive experiments on a real-world dataset show that both approaches are effective and outperform baselines with respect to mining personal context-aware preferences for mobile users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {58},
numpages = {27},
keywords = {personalized recommendation, mobile user, Context aware}
}

@inproceedings{10.1145/2670291.2670292,
author = {Hueting, Moos and Monszpart, Aron and Mellado, Nicolas},
title = {MCGraph: Multi-Criterion Representation for Scene Understanding},
year = {2014},
isbn = {9781450332422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670291.2670292},
doi = {10.1145/2670291.2670292},
abstract = {The field of scene understanding endeavours to extract a broad range of information from 3D scenes. Current approaches exploit one or at most a few different criteria (e.g., spatial, semantic, functional information) simultaneously for analysis. We argue that to take scene understanding to the next level of performance, we need to take into account many different, and possibly previously unconsidered types of knowledge simultaneously. A unified representation for this type of processing is as of yet missing. In this work we propose MCGraph: a unified multi-criterion data representation for understanding and processing of large-scale 3D scenes. Scene abstraction and prior knowledge are kept separated, but highly connected. For this purpose, primitives (i.e., proxies) and their relationships (e.g., contact, support, hierarchical) are stored in an abstraction graph, while the different categories of prior knowledge necessary for processing are stored separately in a knowledge graph. These graphs complement each other bidirectionally, and are processed concurrently. We illustrate our approach by expressing previous techniques using our formulation, and present promising avenues of research opened up by using such a representation. We also distribute a set of MCGraph annotations for a small number of NYU2 scenes, to be used as ground truth multi-criterion abstractions.},
booktitle = {SIGGRAPH Asia 2014 Indoor Scene Understanding Where Graphics Meets Vision},
articleno = {3},
numpages = {9},
keywords = {multi-criteria, scene understanding, scene abstraction},
location = {Shenzhen, China},
series = {SA '14}
}

@inproceedings{10.5555/2693848.2694147,
author = {Park, Jinsoo and Lee, Haneul and So, Byungdu and Kim, Yunbae and Kim, Byung H. and Ko, Keyhoon and Chung, Yeon Jae and Kang, Jiseok and Park, Bum C.},
title = {New Key Performance Indices for Complex Manufacturing Scheduling},
year = {2014},
publisher = {IEEE Press},
abstract = {Diversified and complicated manufacturing sites make optimal scheduling of production lines difficult. Under current manufacturing processes, it is almost impossible for schedulers to consider all the constraints of production processes. A strategy of simulation-based advanced planning and scheduling (APS) is employed to overcome difficulties that interfere with satisfactory on-time delivery and commitment to the current status. In simulation-based scheduling, key performance indices (KPIs) are important for selecting optimal dispatching rules in scheduling. In cases involving complex processes, in which the identification of appropriate KPIs is limited to selection among existing KPIs, KPIs should be chosen and modified carefully to optimize process management and to reflect all of the existing constraints of production. However, the existing methodologies for modifying KPIs are misplaced in complex manufacturing environments such as job-shop processes. We propose a new method to design and select appropriate KPIs that meet the characteristics of any given process, and verify with empirical analysis whether or not the KPIs meet requirements from experts of production lines.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {2384–2395},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.5555/2693848.2694058,
author = {Balaban, Mariusz and Hester, Patrick and Diallo, Saikou},
title = {Towards a Theory of Multi-Method M&amp;S Approach: Part I},
year = {2014},
publisher = {IEEE Press},
abstract = {This paper is the first from a series of papers that aim to develop a theory of multi-method M&amp;S approach. The aim of this paper is to develop ontological basis for multi-method M&amp;S approach. The first part of this paper discusses terms related to the use of more than a single modeling &amp; simulation (M&amp;S) method. This is to show the ontological ambiguity currently present within the M&amp;S field in the context of using more than a single method. Next section provides the philosophical stance of the authors about the main terms in order to provide clarification and context of the term multi-method M&amp;S approach. The last section takes these previous concepts and proposes a set of definitions relevant to a multi-method M&amp;S approach, including its parent and derivative terms.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {1652–1663},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@article{10.14778/2732967.2732969,
author = {Duggan, Jennie},
title = {The Case for Personal Data-Driven Decision Making},
year = {2014},
issue_date = {July 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732967.2732969},
doi = {10.14778/2732967.2732969},
abstract = {Data-driven decision making (D3M) has shown great promise in professional pursuits such as business and government. Here, policymakers collect and analyze data to make their operations more efficient and equitable. Progress in bringing the benefits of D3M to everyday life has been slow. For example, a student asks, "If I pursue an undergraduate degree at this university, what are my expected lifetime earnings?". Presently there is no principled way to search for this, because an accurate answer depends on the student and school.Such queries are personalized, winnowing down large datasets for specific circumstances, rather than applying well-defined predicates. They predict decision outcomes by extrapolating from relevant examples. This vision paper introduces a new approach to D3M that is designed to empower the individual to make informed choices. Here, we highlight research opportunities for the data management community arising from this proposal.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {943–946},
numpages = {4}
}

@inproceedings{10.5555/2616606.2616873,
author = {Mariani, Giovanni and Palermo, Gianluca and Zaccaria, Vittorio and Silvano, Cristina},
title = {DeSpErate: Speeding-up <u>De</u>Sign <u>Sp</u>Ace <u>E</u>Xploration by Using P<u>R</u>Edictive Simul<u>At</u>Ion Sch<u>E</u>Duling},
year = {2014},
isbn = {9783981537024},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {The design space exploration (DSE) phase is used to tune configurable system parameters and it generally consists of a multiobjective optimization (MOO) problem. It is usually done at pre-design phase and consists of the evaluation of large design spaces where each configuration requires long simulation. Several heuristic techniques have been proposed in the past and the recent trend is reducing the exploration time by using analytic prediction models to approximate the system metrics, effectively pruning sub-optimal configurations from the exploration scope. However, there is still a missing path towards the effective usage of the underlying computing resources used by the DSE process. In this work, we will show that an alternative and almost orthogonal approach --- focused on exploiting the available parallelism in terms of computing resources --- can be used to better schedule the simulations and to obtain a high speedup with respect to state of the art approaches, without compromising the accuracy of exploration results. Experimental results will be presented by dealing with the DSE problem of a shared memory multi-core system considering a variable number of available parallel resources to support the DSE phase1.},
booktitle = {Proceedings of the Conference on Design, Automation &amp; Test in Europe},
articleno = {218},
numpages = {4},
location = {Dresden, Germany},
series = {DATE '14}
}

@inproceedings{10.1145/2602087.2602113,
author = {Chen, Qian and Abdelwahed, Sherif},
title = {Towards Realizing Self-Protecting SCADA Systems},
year = {2014},
isbn = {9781450328128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602087.2602113},
doi = {10.1145/2602087.2602113},
abstract = {SCADA (supervisory control and data acquisition) systems are prime cyber attack targets due to potential impacts on properties, economies, and human lives. Current security solutions, such as firewalls, access controls, and intrusion detection and response systems, can protect SCADA systems from cyber assaults (e.g., denial of service attacks, SQL injection attacks, and spoofing attacks), but they are far from perfect. A new technology is emerging to enable self-protection in SCADA systems. Self-protecting SCADA systems are typically an integration of system behavior monitoring, attack estimation and prevention, known and unknown attack detection, live forensics analysis, and system behavior regulation with appropriate responses. This paper first discusses the key components of a self-protecting SCADA system and then surveys the state-of-the-art research and techniques to the realization of such systems.},
booktitle = {Proceedings of the 9th Annual Cyber and Information Security Research Conference},
pages = {105–108},
numpages = {4},
keywords = {self-protection, cybersecurity, autonomic computing},
location = {Oak Ridge, Tennessee, USA},
series = {CISR '14}
}

@inproceedings{10.5555/2693848.2694116,
author = {Ic, Yusuf Tansel and Dengiz, Berna and Dengiz, Orhan and Cizmeci, Gozde},
title = {Topsis Based Taguchi Method for Multi-Response Simulation Optimization of Flexible Manufacturing System},
year = {2014},
publisher = {IEEE Press},
abstract = {This study presents a simulation design and analysis case study of a flexible manufacturing system (FMS) considering a multi-response simulation optimization using TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) based Taguchi approach. While in order to reduce expensive simulation experiments with the Taguchi design, the TOPSIS procedure is used to combine the multiple FMS responses (performance measures) into a single response in the optimization processes. Thus, TOPSIS carries out an important role to build a surrogate objective function that represents multiple responses of the system. The integrated approach finds a new design considering discrete factors (physical and operational parameters) which affect the performance measures of FMS. Optimal design configuration is obtained for the considered system with improved performance.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {2147–2155},
numpages = {9},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1109/UCC.2014.148,
author = {Cayirci, Erdal and Garaga, Alexandr and Santana, Anderson and Roudier, Yves},
title = {A Cloud Adoption Risk Assessment Model},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.148},
doi = {10.1109/UCC.2014.148},
abstract = {Cloud Adoption Risk Assessment Model is designed for cloud customers to assess the risks that they face by selecting a specific cloud service provider. It is an expert system to evaluate various background information obtained from cloud customers, cloud service providers and other public external sources, and to analyze various risk scenarios. This would facilitate cloud customers in making informed decision to select the cloud service provider with the most preferable risk profile.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {908–913},
numpages = {6},
keywords = {risk assessment, cloud computing, security},
series = {UCC '14}
}

@article{10.1145/2567661,
author = {Ulusel, Onur and Nepal, Kumud and Bahar, R. Iris and Reda, Sherief},
title = {Fast Design Exploration for Performance, Power and Accuracy Tradeoffs in FPGA-Based Accelerators},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/2567661},
doi = {10.1145/2567661},
abstract = {The ease-of-use and reconfigurability of FPGAs makes them an attractive platform for accelerating algorithms. However, accelerating becomes a challenging task as the large number of possible design parameters lead to different accelerator variants. In this article, we propose techniques for fast design exploration and multi-objective optimization to quickly identify both algorithmic and hardware parameters that optimize these accelerators. This information is used to run regression analysis and train mathematical models within a nonlinear optimization framework to identify the optimal algorithm and design parameters under various objectives and constraints. To automate and improve the model generation process, we propose the use of L1-regularized least squares regression techniques.We implement two real-time image processing accelerators as test cases: one for image deblurring and one for block matching. For these designs, we demonstrate that by sampling only a small fraction of the design space (0.42% and 1.1%), our modeling techniques are accurate within 2%--4% for area and throughput, 8%--9% for power, and 5%--6% for arithmetic accuracy. We show speedups of 340\texttimes{} and 90\texttimes{} in time for the test cases compared to brute-force enumeration. We also identify the optimal set of parameters for a number of scenarios (e.g., minimizing power under arithmetic inaccuracy bounds).},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = feb,
articleno = {4},
numpages = {22},
keywords = {fast regression analysis, hardware accelerators, Block-matching, design space exploration, image deblur, multi-objective co-exploration, real time image processing}
}

@article{10.1007/s00778-014-0352-3,
author = {Magnani, Matteo and Assent, Ira and Mortensen, Michael L.},
title = {Taking the Big Picture: Representative Skylines Based on Significance and Diversity},
year = {2014},
issue_date = {October   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-014-0352-3},
doi = {10.1007/s00778-014-0352-3},
abstract = {The skyline is a popular operator to extract records from a database when a record scoring function is not available. However, the result of a skyline query can be very large. The problem addressed in this paper is the automatic selection of a small number (k) of representative skyline records. Existing approaches have only focused on partial aspects of this problem. Some try to identify sets of diverse records giving an overall approximation of the skyline. These techniques, however, are sensitive to the scaling of attributes or to the insertion of non-skyline records into the database. Others exploit some knowledge of the record scoring function to identify the most significant record, but not sets of records representative of the whole skyline. In this paper, we introduce a novel approach taking both the significance of all the records and their diversity into account, adapting to available knowledge of the scoring function, but also working under complete ignorance. We show the intractability of the problem and present approximate algorithms. We experimentally show that our approach is efficient, scalable and that it improves existing works in terms of the significance and diversity of the results.},
journal = {The VLDB Journal},
month = oct,
pages = {795–815},
numpages = {21},
keywords = {Representative skyline, Diversity, Significance}
}

@inproceedings{10.5555/2664323.2664347,
author = {Gerber, David Jason and Shiordia, Rodrigo and Veetil, Sreerag and Mahesh, Arjun},
title = {Design Agency: Prototyping Multi-Agent System Simulation for Design Search and Exploration},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {This work presents prototypes of multi-agent system simulation for design search and exploration. We describe an experimental approach in part based on a previously established multidisciplinary design optimization (MDO) framework. Here the work further explores the potential impact of MDO in conjunction with multi-agent systems on the early stages of design. Specifically, this paper addresses the potential of introducing agent based computing techniques into the multidisciplinary architectural design optimization and search workflow to tackle geometrically complex design problems and to facilitate early stage design exploration. To address these interests a series of prototyped workflows were studied inclusive of environmental performance and structural performance metrics and benchmarks. This paper presents a novel methodology for using simulation data in conjunction with multi-agent systems as a way for re-informing form and enhancing performance in a generative design environment. The methodology is based on the use of swarm algorithms and their integration with data generated by simulation software. The interaction between these two domains, the simulation data and swarm algorithms, generates the final outputs as a modified geometry that is then evaluated by comparison for enhanced design performance.},
booktitle = {Proceedings of the Symposium on Simulation for Architecture &amp; Urban Design},
articleno = {24},
numpages = {8},
keywords = {parametric design, generative design, multi-objective optimization (MOO), multi-agent systems, multidisciplinary design optimization (MDO)},
location = {Tampa, Florida},
series = {SimAUD '14}
}

@inproceedings{10.5555/2664323.2664349,
author = {Bradner, Erin and Iorio, Francesco and Davis, Mark},
title = {Parameters Tell the Design Story: Ideation and Abstraction in Design Optimization},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {We report qualitative findings from interviews and observations detailing how professionals generate and evaluate design ideas using design optimization tools. We interviewed 18 architects and manufacturing design professionals. We frame our findings using the Geneplore model of creative cognition and classify examples of ideation and abstract design thinking arising from optimization workflows. Contrary to our expectations, we found that the computed optimum was often used as the starting point for design exploration, not the end product. We also found that parametric models, plus their associated parameters and simulations serve as an alternate, highly valued form of design documentation distinct from engineering schematics.},
booktitle = {Proceedings of the Symposium on Simulation for Architecture &amp; Urban Design},
articleno = {26},
numpages = {8},
keywords = {engineering design, ideation, optioneering, qualitative research, architectural design, design optimization},
location = {Tampa, Florida},
series = {SimAUD '14}
}

