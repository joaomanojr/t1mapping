@article{10.1145/3408314,
author = {Davoudian, Ali and Liu, Mengchi},
title = {Big Data Systems: A Software Engineering Perspective},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408314},
doi = {10.1145/3408314},
abstract = {Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {110},
numpages = {39},
keywords = {Big Data, software engineering, requirements engineering, software reference architecture, quality assurance, Big Data systems}
}

@inproceedings{10.1145/3377929.3389939,
author = {Talukder, A K M Khaled Ahsan and Deb, Kalyanmoy},
title = {PaletteStarViz: A Visualization Method for Multi-Criteria Decision Making from High-Dimensional Pareto-Optimal Front},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389939},
doi = {10.1145/3377929.3389939},
abstract = {Visual representation of a many-objective Pareto-optimal front in four or more dimensional objective space requires a large number of data points. Moreover, choosing a single point from a large set even with certain preference information is problematic, as it causes a large cognitive burden on the part of the decision-makers. Therefore, many-objective optimization and decision-making practitioners have been interested in effective visualization methods to enable them to filter down a large set to a few critical points for further analysis. Most existing visualization methods are borrowed from other data analytic domains and they are too generic to be effective for many-criteria decision making. In this paper, we propose an alternative visualization method, following an earlier concept, using star-coordinate plots for effectively visualizing many-objective trade-off solutions. The proposed PaletteStarViz respects some basic topological, geometric, and functional decision-making properties of high-dimensional trade-off points mapped to a "two-and-a-half" dimensional space. We demonstrate the use of PaletteStarViz to a number high-dimensional Pareto-optimal fronts.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {175–176},
numpages = {2},
keywords = {visualization, high-dimensional pareto-optimal front, many-objective optimization, multi-criteria decision making},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3372224.3419194,
author = {Laskaridis, Stefanos and Venieris, Stylianos I. and Almeida, Mario and Leontiadis, Ilias and Lane, Nicholas D.},
title = {SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419194},
doi = {10.1145/3372224.3419194},
abstract = {Despite the soaring use of convolutional neural networks (CNNs) in mobile applications, uniformly sustaining high-performance inference on mobile has been elusive due to the excessive computational demands of modern CNNs and the increasing diversity of deployed devices. A popular alternative comprises offloading CNN processing to powerful cloud-based servers. Nevertheless, by relying on the cloud to produce outputs, emerging mission-critical and high-mobility applications, such as drone obstacle avoidance or interactive applications, can suffer from the dynamic connectivity conditions and the uncertain availability of the cloud. In this paper, we propose SPINN, a distributed inference system that employs synergistic device-cloud computation together with a progressive inference method to deliver fast and robust CNN inference across diverse settings. The proposed system introduces a novel scheduler that co-optimises the early-exit policy and the CNN splitting at run time, in order to adapt to dynamic conditions and meet user-defined service-level requirements. Quantitative evaluation illustrates that SPINN outperforms its state-of-the-art collaborative inference counterparts by up to 2\texttimes{} in achieved throughput under varying network conditions, reduces the server cost by up to 6.8\texttimes{} and improves accuracy by 20.7% under latency constraints, while providing robust operation under uncertain connectivity conditions and significant energy savings compared to cloud-centric execution.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {37},
numpages = {15},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.1145/3416921.3416925,
author = {Zhao, Weisong and Cui, Xiaoyu},
title = {A Fast Adaptive Replica Recovery Algorithm Based on Access Frequency and Environment Awareness},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416925},
doi = {10.1145/3416921.3416925},
abstract = {As cloud storage adopts a distributed architecture to store massive data, how to improve the reliability of the storage center has become the focus of researchers. HDFS, the distributed file system of Hadoop, uses a sequential recovery method to recover failed replicas when the node is down, which does not take into account the priority of the replicas and the load difference between each node, resulting in request blocking and system load imbalance. Aiming at this problem, we propose a replica recovery method based on access frequency and network environment awareness, use a priority-based recovery algorithm and multi-objective decision algorithm to guarantee the response speed and realize the cluster load balance. We set up a simulation environment for verification and compared the performance of our method with various algorithms in throughput and response time. The simulation results showed that our approach effectively solved the load imbalance problem on the premise of ensuring faster response time.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {102–107},
numpages = {6},
keywords = {storage reliability, replica recovery, HDFS},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@inproceedings{10.1145/3375627.3375862,
author = {Zhang, Yunfeng and Bellamy, Rachel and Varshney, Kush},
title = {Joint Optimization of AI Fairness and Utility: A Human-Centered Approach},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375862},
doi = {10.1145/3375627.3375862},
abstract = {Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {400–406},
numpages = {7},
keywords = {policy elicitation, algorithmic fairness, multi-criteria decision making},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.5555/3398761.3399109,
author = {Radulescu, Roxana and Mannion, Patrick and Roijers, Diederik M. and Now\'{e}, Ann},
title = {Multi-Objective Multi-Agent Decision Making: A Utility-Based Analysis and Survey},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many real-world decision problems are inherently multi-objective in nature and concern multiple actors, making multi-objective multi-agent systems a key domain to study. We argue that trade-offs between conflicting objective functions should be analysed on the basis of the utility that these trade-offs have for the users of a system. We develop a new taxonomy which classifies multi-objective multi-agent decision making settings, on the basis of the reward structures and utility functions. We analyse which solution concepts apply to the different settings in our taxonomy, which allows us to offer a structured view of the field and identify promising directions for future research.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2158–2160},
numpages = {3},
keywords = {multi-objective decision making, reinforcement learning, solution concepts, multi-objective optimisation criteria, multi-agent systems},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3398761.3399081,
author = {Zhang, Yijie and R\u{a}dulescu, Roxana and Mannion, Patrick and Roijers, Diederik M. and Now\'{e}, Ann},
title = {Opponent Modelling for Reinforcement Learning in Multi-Objective Normal Form Games},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we investigate the effects of opponent modelling on multi-objective multi-agent interactions with non-linear utilities. Specifically, we consider multi-objective normal form games (MONFGs) with non-linear utility functions under the scalarised expected returns optimisation criterion. We contribute a novel actor-critic formulation to allow reinforcement learning of mixed strategies in this setting, along with an extension that incorporates opponent policy reconstruction using conditional action frequencies. Our empirical results demonstrate that opponent modelling can drastically alter the learning dynamics in this setting.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2080–2082},
numpages = {3},
keywords = {reinforcement learning, multi-objective decision making, nash equilibrium, game theory, multi-agent systems, opponent modelling},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3398761.3399127,
author = {R\u{a}dulescu, Roxana},
title = {A Utility-Based Perspective on Multi-Objective Multi-Agent Decision Making},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Numerous real-world problems involve multiple interacting entities and are inherently multi-objective in nature. Multi-objective multi-agent systems are a suitable paradigm to model such settings Despite the rising interest in this field, it has become difficult to compare or categorise approaches and identify the state-of-the-art solutions. Therefore, our first contribution is to develop a new taxonomy on the basis of the reward structures and utility functions, to offer a more structured view of the field. We note that utility functions are usually modelled as weights that define preferences over objectives, despite the fact that in many problems this assumption is not valid. We analyse the effect of non-linear utility functions on the set of equilibria in general multi-objective normal form games, under different optimisation criteria and look at how opponent modelling can aid the learning process in this setting. For future work, we are interested in how sequential settings can be approached under these considerations, to get a step closer to creating hybrid, artificial-and-human, multi-agent collectives that can deal with the different preferences w.r.t. the objectives of the different agents in the collective.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2207–2208},
numpages = {2},
keywords = {multi-objective optimisation criteria, multi-agent systems, multi-objective decision making, solution concepts, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3394486.3403245,
author = {Boominathan, Soorajnath and Oberst, Michael and Zhou, Helen and Kanjilal, Sanjat and Sontag, David},
title = {Treatment Policy Learning in Multiobjective Settings with Fully Observed Outcomes},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403245},
doi = {10.1145/3394486.3403245},
abstract = {In several medical decision-making problems, such as antibiotic prescription, laboratory testing can provide precise indications for how a patient will respond to different treatment options. This enables us to "fully observe" all potential treatment outcomes, but while present in historical data, these results are infeasible to produce in real-time at the point of the initial treatment decision. Moreover, treatment policies in these settings often need to trade off between multiple competing objectives, such as effectiveness of treatment and harmful side effects. We present, compare, and evaluate three approaches for learning individualized treatment policies in this setting: First, we consider two indirect approaches, which use predictive models of treatment response to construct policies optimal for different trade-offs between objectives. Second, we consider a direct approach that constructs such a set of policies without intermediate models of outcomes. Using a medical dataset of Urinary Tract Infection (UTI) patients, we show that all approaches learn policies that achieve strictly better performance on all outcomes than clinicians, while also trading off between different objectives. We demonstrate additional benefits of the direct approach, including flexibly incorporating other goals such as deferral to physicians on simple cases.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1937–1947},
numpages = {11},
keywords = {learning to defer, machine learning, multi-objective optimization, policy learning, antibiotics, healthcare, decision making},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3386762.3388955,
author = {B\"{u}y\"{u}k\"{o}zkan, G\"{u}l\c{c}in and Uzt\"{u}rk, Deniz},
title = {Fleet Vehicle Selection for Sustainable Urban Logistics},
year = {2020},
isbn = {9781450376891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386762.3388955},
doi = {10.1145/3386762.3388955},
abstract = {Nowadays, urban logistics becomes a critical subject due to the extension of the population in cities. Urban transportation, which is a sub-category of urban logistics, has a massive connection with city residents; hence, it needed to be evaluated with the residents' needs to achieve a sustainable and efficient process. Last-mile delivery in which the micro-level transportation operations are included in urban areas is also another crucial area that affects the resident's life. To impress on its efficiency and create durable cities, this paper concentrates on a selection problem for the last-mile delivery area. The selection is approached as a multi-criteria decision-making (MCDM) process. In order to enable the methodology to deal with linguistic variables, 2-Tuple linguistic-based SAW-VIKOR techniques are suggested for this problem. To test the validity of the suggested methods, a case study is applied with a cargo company from Turkey.},
booktitle = {Proceedings of the 2020 The 9th International Conference on Informatics, Environment, Energy and Applications},
pages = {116–120},
numpages = {5},
keywords = {Sustainability, 2-Tuple linguistic model, Last-mile delivery, Urban logistics, MCDM},
location = {Amsterdam, Netherlands},
series = {IEEA 2020}
}

@article{10.1145/3395046,
author = {Zhou, Xinyi and Zafarani, Reza},
title = {A Survey of Fake News: Fundamental Theories, Detection Methods, and Opportunities},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3395046},
doi = {10.1145/3395046},
abstract = {The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news detection and intervention. This survey reviews and evaluates methods that can detect fake news from four perspectives: the false knowledge it carries, its writing style, its propagation patterns, and the credibility of its source. The survey also highlights some potential research tasks based on the review. In particular, we identify and detail related fundamental theories across various disciplines to encourage interdisciplinary research on fake news. It is our hope that this survey can facilitate collaborative efforts among experts in computer and information sciences, social sciences, political science, and journalism to research fake news, where such efforts can lead to fake news detection that is not only efficient but, more importantly, explainable.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {109},
numpages = {40},
keywords = {knowledge graph, fact-checking, deception detection, news verification, disinformation, social media, misinformation, information credibility, Fake news}
}

@inproceedings{10.1145/3384544.3384563,
author = {Hikmawati, Erna and Surendro, Kridanto},
title = {How to Determine Minimum Support in Association Rule},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384563},
doi = {10.1145/3384544.3384563},
abstract = {The growth of increasingly complex data now raises new challenges in the world of technology. Large volumes of data store a lot of knowledge that can help in the decision-making process. One way to find knowledge in big data is by the Association Rule. The association rule is a technique in data mining that can produce rules based on the frequency of items appearing from a transactional database. One thing that is critical in the association rule is the determination of the minimum support value used to determine which items will be included in the formation of rules. If the minimum support value that is set is too small, it causes too many items to be involved in establishing the rules. Conversely, if the minimum support is too large, the number of items involved in forming the rule is too small. The problem in determining the minimum support value greatly affects the accuracy of the resulting rule. In this paper, various methods will be discussed to determine the minimum value of support through study literature based on related research. In addition, it explains research opportunities that can be done in the future about the minimum value of support determination in the association rule.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {6–10},
numpages = {5},
keywords = {Minimum Support, Data Mining, Association Rule},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@article{10.1145/3409481.3409485,
author = {Lissandrini, Matteo and Pedersen, Torben Bach and Hose, Katja and Mottin, Davide},
title = {Knowledge Graph Exploration: Where Are We and Where Are We Going?},
year = {2020},
issue_date = {Summer 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {Summer 2020},
issn = {1931-1745},
url = {https://doi.org/10.1145/3409481.3409485},
doi = {10.1145/3409481.3409485},
abstract = {Knowledge graphs (KGs) represent facts in the form of subject-predicate-object triples and are widely used to represent and share knowledge on the Web. Their ability to represent data in complex domains augmented with semantic annotations has attracted the attention of both research and industry. Yet, their widespread adoption in various domains and their generation processes have made the contents of these resources complicated. We speak of knowledge graph exploration as of the gradual discovery and understanding of the contents of a large and unfamiliar KG. In this paper, we present an overview of the state-of-the-art approaches for KG exploration. We divide them into three areas: profiling, search, and analysis and we argue that, while KG profiling and KG exploratory search received considerable attention, exploratory KG analytics is still in its infancy. We conclude with an overview of promising future research directions towards the design of more advanced KG exploration techniques.},
journal = {SIGWEB Newsl.},
month = jul,
articleno = {4},
numpages = {8}
}

@inproceedings{10.1145/3336191.3371768,
author = {Prabhu, Yashoteja and Kusupati, Aditya and Gupta, Nilesh and Varma, Manik},
title = {Extreme Regression for Dynamic Search Advertising},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371768},
doi = {10.1145/3336191.3371768},
abstract = {This paper introduces a new learning paradigm called eXtreme Regression (XR) whose objective is to accurately predict the numerical degrees of relevance of an extremely large number of labels to a data point. XR can provide elegant solutions to many large-scale ranking and recommendation applications including Dynamic Search Advertising (DSA). XR can learn more accurate models than the recently popular extreme classifiers which incorrectly assume strictly binary-valued label relevances. Traditional regression metrics which sum the errors over all the labels are unsuitable for XR problems since they could give extremely loose bounds for the label ranking quality. Also, the existing regression algorithms won't efficiently scale to millions of labels. This paper addresses these limitations through: (1) new evaluation metrics for XR which sum only the k largest regression errors; (2) a new algorithm called XReg which decomposes XR task into a hierarchy of much smaller regression problems thus leading to highly efficient training and prediction. This paper also introduces a (3) new labelwise prediction algorithm in XReg useful for DSA and other recommendation tasks.Experiments on benchmark datasets demonstrated that XReg can outperform the state-of-the-art extreme classifiers as well as large-scale regressors and rankers by up to 50% reduction in the new XR error metric, and up to 2% and 2.4% improvements in terms of the propensity-scored precision metric used in extreme classification and the click-through rate metric used in DSA respectively. Deployment of XReg on DSA in Bing resulted in a relative gain of 58% in revenue and 27% in query coverage. XReg's source code can be downloaded from http://manikvarma.org/code/Xreg/download.html.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {456–464},
numpages = {9},
keywords = {extreme classification, regression, dynamic search advertising},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.5555/3408352.3408644,
author = {Chen, Weiwei and Wang, Ying and Yang, Shuang and Liu, Chen and Zhang, Lei},
title = {You Only Search Once: A Fast Automation Framework for Single-Stage DNN/Accelerator Co-Design},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {DNN/Accelerator co-design has shown great potential in improving QoR and performance. Typical approaches separate the design flow into two-stage: (1) designing an application-specific DNN model with high accuracy; (2) building an accelerator considering the DNN specific characteristics. However, it may fails in promising the highest composite score which combines the goals of accuracy and other hardware-related constraints (e.g., latency, energy efficiency) when building a specific neural-network-based system. In this work, we present a single-stage automated framework, YOSO, aiming to generate the optimal solution of software-and-hardware that flexibly balances between the goal of accuracy, power, and QoS. Compared with the two-stage method on the baseline systolic array accelerator and Cifar10 dataset, we achieve 1.42x~2.29x energy or 1.79x~3.07x latency reduction at the same level of precision, for different user-specified energy and latency optimization constraints, respectively.Keywords-Automl, Hardware/Software co-design, Acceleration.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1283–1286},
numpages = {4},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.1145/3397166.3413467,
author = {Ucar, Seyhan and Higuchi, Takamasa and Wang, Chang-Heng and Deveaux, Duncan and H\"{a}rri, J\'{e}r\^{o}me and Altintas, Onur},
title = {Vehicular Knowledge Networking and Application to Risk Reasoning},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3413467},
doi = {10.1145/3397166.3413467},
abstract = {Vehicles are expected to generate and consume an increasing amount of data, but how to perform risk reasoning over relevant data is still not yet solved. Location, time of day and driver behavior change the risk dynamically and make risk assessment challenging. This paper introduces a new paradigm, transferring information from raw sensed data to knowledge and explores the knowledge of risk reasoning through vehicular maneuver conflicts. In particular, we conduct a simulation study to analyze the driving data and extract the knowledge of risky road users and risky locations. We use knowledge to facilitate reduced volume and share it through a Vehicular Knowledge Network (VKN) for better traffic planning and safer driving.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {351–356},
numpages = {6},
keywords = {risk reasoning, vehicular knowledge networking, knowledge},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1145/3366423.3380200,
author = {Savage, Saiph and Chiang, Chun Wei and Saito, Susumu and Toxtli, Carlos and Bigham, Jeffrey},
title = {Becoming the Super Turker:Increasing Wages via a Strategy from High Earning Workers},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380200},
doi = {10.1145/3366423.3380200},
abstract = {Crowd markets have traditionally limited workers by not providing transparency information concerning which tasks pay fairly or which requesters are unreliable. Researchers believe that a key reason why crowd workers earn low wages is due to this lack of transparency. As a result, tools have been developed to provide more transparency within crowd markets to help workers. However, while most workers use these tools, they still earn less than minimum wage. We argue that the missing element is guidance on how to use transparency information. In this paper, we explore how novice workers can improve their earnings by following the transparency criteria of Super Turkers, i.e., crowd workers who earn higher salaries on Amazon Mechanical Turk (MTurk). We believe that Super Turkers have developed effective processes for using transparency information. Therefore, by having novices follow a Super Turker criteria (one that is simple and popular among Super Turkers), we can help novices increase their wages. For this purpose, we: (i) conducted a survey and data analysis to computationally identify a simple yet common criteria that Super Turkers use for handling transparency tools; (ii) deployed a two-week field experiment with novices who followed this Super Turker criteria to find better work on MTurk. Novices in our study viewed over 25,000 tasks by 1,394 requesters. We found that novices who utilized this Super Turkers’ criteria earned better wages than other novices. Our results highlight that tool development to support crowd workers should be paired with educational opportunities that teach workers how to effectively use the tools and their related metrics (e.g., transparency values). We finish with design recommendations for empowering crowd workers to earn higher salaries.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1241–1252},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3378935,
author = {Cong, Peijin and Zhou, Junlong and Li, Liying and Cao, Kun and Wei, Tongquan and Li, Keqin},
title = {A Survey of Hierarchical Energy Optimization for Mobile Edge Computing: A Perspective from End Devices to the Cloud},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3378935},
doi = {10.1145/3378935},
abstract = {With the development of wireless technology, various emerging mobile applications are attracting significant attention and drastically changing our daily lives. Applications such as augmented reality and object recognition demand stringent delay and powerful processing capability, which exerts enormous pressure on mobile devices with limited resources and energy. In this article, a survey of techniques for mobile device energy optimization is presented in a hierarchy of device design and operation, computation offloading, wireless data transmission, and cloud execution of offloaded computation. Energy management strategies for mobile devices from hardware and software aspects are first discussed, followed by energy-efficient computation offloading frameworks for mobile applications that trade application response time for device energy consumption. Then, techniques for efficient wireless data communication to reduce transmission energy are summarized. Finally, the execution mechanisms of application components or tasks in various clouds are further described to provide energy-saving opportunities for mobile devices. We classify the research works based on key characteristics of devices and applications to emphasize their similarities and differences. We hope that this survey will give insights to researchers into energy management mechanisms on mobile devices, and emphasize the crucial importance of optimizing device energy consumption for more research efforts in this area.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {38},
numpages = {44},
keywords = {mobile edge computing (MEC), Computation offloading, mobile computing (MC), wireless communication, energy optimization, mobile devices, mobile cloud computing (MCC)}
}

@inproceedings{10.1145/3340631.3394845,
author = {Mauro, Noemi and Ardissono, Liliana and Cena, Federica},
title = {Personalized Recommendation of PoIs to People with Autism},
year = {2020},
isbn = {9781450368612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340631.3394845},
doi = {10.1145/3340631.3394845},
abstract = {The suggestion of Points of Interest to people with Autism Spectrum Disorder (ASD) challenges recommender systems research because these users' perception of places is influenced by idiosyncratic sensory aversions which can mine their experience by causing stress and anxiety. Therefore, managing individual preferences is not enough to provide these people with suitable recommendations. In order to address this issue, we propose a Top-N recommendation model that combines the user's idiosyncratic aversions with her/his preferences in a personalized way to suggest the most compatible and likable Points of Interest for her/him. We are interested in finding a user-specific balance of compatibility and interest within a recommendation model that integrates heterogeneous evaluation criteria to appropriately take these aspects into account. We tested our model on both ASD and "neurotypical" people. The evaluation results show that, on both groups, our model outperforms in accuracy and ranking capability the recommender systems based on item compatibility, on user preferences, or which integrate these two aspects by means of a uniform evaluation model.},
booktitle = {Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {163–172},
numpages = {10},
keywords = {autism spectrum disorder, accessibility, recommender systems},
location = {Genoa, Italy},
series = {UMAP '20}
}

@inproceedings{10.1145/3387906.3388630,
author = {Alfayez, Reem and Alwehaibi, Wesam and Winn, Robert and Venson, Elaine and Boehm, Barry},
title = {A Systematic Literature Review of Technical Debt Prioritization},
year = {2020},
isbn = {9781450379601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387906.3388630},
doi = {10.1145/3387906.3388630},
abstract = {Repaying all technical debt (TD) present in a system may be unfeasible, as there is typically a shortage in the resources allocated for TD repayment. Therefore, TD prioritization is essential to best allocate such resources to determine which TD items are to be repaid first and which items are to be delayed until later releases. This study conducts a systematic literature review (SLR) to identify and analyze the currently researched TD prioritization approaches. The employed search strategy strove to achieve high completeness through the identification of a quasi-gold standard set, which was used to establish a search string to automatically retrieve papers from select research databases. The application of selection criteria, along with forward and backward snowballing, identified 24 TD prioritization approaches. The analysis of the identified approaches revealed a scarcity of approaches that account for cost, value, and resources constraint and a lack of industry evaluation. Furthermore, this SLR unveils potential gaps in the current TD prioritization research, which future research may explore.},
booktitle = {Proceedings of the 3rd International Conference on Technical Debt},
pages = {1–10},
numpages = {10},
keywords = {software maintenance, software, prioritization, technical debt, software management},
location = {Seoul, Republic of Korea},
series = {TechDebt '20}
}

